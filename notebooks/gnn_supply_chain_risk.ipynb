{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "title_and_objectives"
      },
      "source": [
        "# GNN Supply Chain Risk Analysis\n",
        "\n",
        "This notebook implements a **Graph Neural Network (GNN)** for supply chain risk analysis using PyTorch Geometric.\n",
        "\n",
        "## Why Graph Neural Networks for Supply Chain?\n",
        "\n",
        "Traditional risk models treat suppliers independently, missing critical **network effects**:\n",
        "- A Tier-2 supplier failure can cascade through multiple Tier-1 suppliers\n",
        "- Geographic concentration creates correlated risks\n",
        "- Hidden dependencies exist in trade data that aren't in master data\n",
        "\n",
        "**GNNs solve this** by learning from the graph structure itself. Each node (supplier, material, region) learns a representation that incorporates information from its neighbors through **message passing**.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "| Term | Definition |\n",
        "|------|------------|\n",
        "| **Heterogeneous Graph** | A graph with multiple node types (Vendor, Material, Region) and edge types (SUPPLIES, LOCATED_IN) |\n",
        "| **GraphSAGE** | \"Graph Sample and Aggregate\" - a GNN that learns by sampling neighbors and aggregating their features |\n",
        "| **Link Prediction** | Predicting whether an edge should exist between two nodes (used to discover hidden Tier-2 relationships) |\n",
        "| **Node Embedding** | A learned vector representation of a node that captures its position and role in the network |\n",
        "| **Message Passing** | The mechanism by which nodes exchange information with neighbors across edges |\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. **Build a heterogeneous graph** from supply chain data (Vendors, Materials, Regions, External Suppliers)\n",
        "2. **Train a GraphSAGE model** for link prediction to discover hidden Tier-2 dependencies\n",
        "3. **Propagate risk scores** through the network using learned embeddings\n",
        "4. **Identify bottlenecks** (single points of failure) where many vendors depend on one external supplier\n",
        "5. **Write results** back to Snowflake tables for downstream analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "environment_setup_header"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Install PyTorch Geometric and dependencies. Uses `os.system` for compatibility with headless execution on SPCS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "install_packages"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# =============================================================================\n",
        "# PACKAGE INSTALLATION\n",
        "# =============================================================================\n",
        "# PyTorch Geometric (PyG) is the leading library for deep learning on graphs.\n",
        "# It provides efficient implementations of GNN layers like GraphSAGE, GAT, GCN.\n",
        "#\n",
        "# Key packages:\n",
        "# - torch: PyTorch deep learning framework (tensors, autograd, neural networks)\n",
        "# - torch-geometric: GNN layers, graph data structures, and utilities\n",
        "# - torch-scatter: Efficient scatter operations for aggregating neighbor messages\n",
        "# - torch-sparse: Sparse matrix operations for large graphs\n",
        "#\n",
        "# We use os.system() instead of !pip because Snowflake notebook headless \n",
        "# execution doesn't support shell commands via the ! prefix.\n",
        "# =============================================================================\n",
        "\n",
        "packages = [\n",
        "    \"torch\",\n",
        "    \"torch-geometric\",\n",
        "    \"torch-scatter\",\n",
        "    \"torch-sparse\",\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    print(f\"Installing {pkg}...\")\n",
        "    os.system(f\"{sys.executable} -m pip install {pkg} -q\")\n",
        "\n",
        "print(\"[OK] Package installation complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "import_libraries"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LIBRARY IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "# Standard data science stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PyTorch: The deep learning framework\n",
        "# - torch.nn: Neural network modules (layers, loss functions)\n",
        "# - torch.nn.functional (F): Stateless functions (ReLU, dropout, softmax)\n",
        "# - torch.optim: Optimizers (Adam, SGD) for gradient descent\n",
        "# -----------------------------------------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PyTorch Geometric: Graph Neural Network library\n",
        "# - HeteroData: Data structure for heterogeneous graphs (multiple node/edge types)\n",
        "# - SAGEConv: GraphSAGE convolution layer - aggregates neighbor features\n",
        "# - HeteroConv: Wrapper to apply different convolutions per edge type\n",
        "# - Linear: Linear transformation layer for projecting features\n",
        "# - ToUndirected: Adds reverse edges (A->B becomes A<->B) for bidirectional \n",
        "#                 message passing - critical for learning from relationships\n",
        "# -----------------------------------------------------------------------------\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import SAGEConv, HeteroConv, Linear\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "# Visualization libraries for model diagnostics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Snowflake session for data access\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "# Report compute environment\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "snowflake_session_setup"
      },
      "outputs": [],
      "source": [
        "# Get Snowflake session\n",
        "session = get_active_session()\n",
        "\n",
        "# Configuration\n",
        "DATABASE = \"GNN_SUPPLY_CHAIN_RISK\"\n",
        "SCHEMA = \"GNN_SUPPLY_CHAIN_RISK\"\n",
        "\n",
        "# Version tracking enables A/B testing different model configs and rolling back if a deployment fails\n",
        "MODEL_VERSION = \"v1.0.0\"\n",
        "\n",
        "# Set context\n",
        "session.sql(f\"USE DATABASE {DATABASE}\").collect()\n",
        "session.sql(f\"USE SCHEMA {SCHEMA}\").collect()\n",
        "\n",
        "print(f\"[OK] Connected to {DATABASE}.{SCHEMA}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "load_data_header"
      },
      "source": [
        "## 2. Load Data from Snowflake\n",
        "\n",
        "Load the supply chain data from Snowflake tables into pandas DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "load_snowflake_tables"
      },
      "outputs": [],
      "source": [
        "# Load data from Snowflake tables\n",
        "print(\"Loading data from Snowflake...\")\n",
        "\n",
        "vendors_df = session.table(\"VENDORS\").to_pandas()\n",
        "materials_df = session.table(\"MATERIALS\").to_pandas()\n",
        "purchase_orders_df = session.table(\"PURCHASE_ORDERS\").to_pandas()\n",
        "bom_df = session.table(\"BILL_OF_MATERIALS\").to_pandas()\n",
        "\n",
        "# Trade data reveals hidden Tier-2 suppliers not in master data via shipping records\n",
        "trade_data_df = session.table(\"TRADE_DATA\").to_pandas()\n",
        "regions_df = session.table(\"REGIONS\").to_pandas()\n",
        "\n",
        "print(f\"[OK] Loaded data:\")\n",
        "print(f\"  - Vendors: {len(vendors_df)}\")\n",
        "print(f\"  - Materials: {len(materials_df)}\")\n",
        "print(f\"  - Purchase Orders: {len(purchase_orders_df)}\")\n",
        "print(f\"  - BOM: {len(bom_df)}\")\n",
        "print(f\"  - Trade Data: {len(trade_data_df)}\")\n",
        "print(f\"  - Regions: {len(regions_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "build_graph_header"
      },
      "source": [
        "## 3. Build Heterogeneous Graph\n",
        "\n",
        "A **heterogeneous graph** has multiple types of nodes and edges. This is essential for supply chain modeling where relationships are diverse.\n",
        "\n",
        "### Graph Schema\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     SUPPLIES      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚    VENDOR    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   MATERIAL   â”‚\n",
        "â”‚  (Tier 1-3)  â”‚                   â”‚  (Parts/Raw) â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "       â”‚                                  â”‚\n",
        "       â”‚ LOCATED_IN                       â”‚ COMPONENT_OF\n",
        "       â–¼                                  â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚    REGION    â”‚                   â”‚   MATERIAL   â”‚\n",
        "â”‚  (Country)   â”‚                   â”‚   (Parent)   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "       â–²\n",
        "       â”‚ (inferred via trade)\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     SHIPS_TO      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   EXTERNAL   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    VENDOR    â”‚\n",
        "â”‚  (Shipper)   â”‚                   â”‚              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "- **SUPPLIES** edges: Which vendors supply which materials (from purchase orders)\n",
        "- **COMPONENT_OF** edges: Bill of materials hierarchy (child â†’ parent)\n",
        "- **LOCATED_IN** edges: Geographic risk exposure (vendor â†’ country)\n",
        "- **SHIPS_TO** edges: Trade data reveals hidden Tier-2 relationships\n",
        "\n",
        "The GNN will learn to propagate information across ALL these edge types simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "create_node_mappings"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NODE MAPPINGS: Converting IDs to Graph Indices\n",
        "# =============================================================================\n",
        "# PyTorch Geometric represents nodes as contiguous integer indices [0, 1, 2, ...]\n",
        "# We need to map business IDs (like \"VENDOR_001\") to these indices.\n",
        "#\n",
        "# This is analogous to creating a vocabulary in NLP - each unique entity gets\n",
        "# a unique integer ID that we'll use throughout the model.\n",
        "# =============================================================================\n",
        "\n",
        "def create_node_mappings(vendors_df, materials_df, regions_df, trade_data_df):\n",
        "    \"\"\"\n",
        "    Create bidirectional mappings from business IDs to graph indices.\n",
        "    \n",
        "    Returns a dict of dicts:\n",
        "        mappings['vendor']['VENDOR_001'] = 0  # ID to index\n",
        "        \n",
        "    We'll create reverse mappings later when outputting results.\n",
        "    \"\"\"\n",
        "    # Each unique vendor gets an index 0, 1, 2, ...\n",
        "    vendor_to_idx = {v: i for i, v in enumerate(vendors_df['VENDOR_ID'].unique())}\n",
        "    \n",
        "    # Same for materials (parts, raw materials, finished goods)\n",
        "    material_to_idx = {m: i for i, m in enumerate(materials_df['MATERIAL_ID'].unique())}\n",
        "    \n",
        "    # Regions (countries) for geographic risk\n",
        "    region_to_idx = {r: i for i, r in enumerate(regions_df['REGION_CODE'].unique())}\n",
        "    \n",
        "    # External suppliers are discovered from trade/shipping data\n",
        "    # These are potential Tier-2+ suppliers not in our master vendor list\n",
        "    # This is the key innovation - discovering unknown suppliers from shipping patterns\n",
        "    external_suppliers = trade_data_df['SHIPPER_NAME'].unique()\n",
        "    external_to_idx = {e: i for i, e in enumerate(external_suppliers)}\n",
        "    \n",
        "    return {\n",
        "        'vendor': vendor_to_idx,\n",
        "        'material': material_to_idx,\n",
        "        'region': region_to_idx,\n",
        "        'external': external_to_idx\n",
        "    }\n",
        "\n",
        "mappings = create_node_mappings(vendors_df, materials_df, regions_df, trade_data_df)\n",
        "\n",
        "print(\"Node mappings created:\")\n",
        "print(\"=\" * 40)\n",
        "for k, v in mappings.items():\n",
        "    print(f\"  {k:12s}: {len(v):5d} nodes\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"  TOTAL      : {sum(len(v) for v in mappings.values()):5d} nodes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "node_features_explainer"
      },
      "source": [
        "### Understanding Node Features in Graph Neural Networks\n",
        "\n",
        "Unlike traditional ML where each sample is independent, in GNNs the **features are just the starting point**. The model will transform these initial features by combining them with neighbor information.\n",
        "\n",
        "**What makes good node features?**\n",
        "\n",
        "1. **Intrinsic properties**: Attributes of the node itself (financial health, criticality score)\n",
        "2. **Categorical indicators**: One-hot encodings help the model learn type-specific patterns\n",
        "3. **Normalized scales**: All features should be in similar ranges (typically [0,1]) so no single feature dominates\n",
        "\n",
        "**A key insight**: The GNN will learn to combine features from connected nodes. A vendor's final embedding will incorporate:\n",
        "- Its own features (financial health, tier)\n",
        "- Features of materials it supplies (criticality)\n",
        "- Features of its region (geopolitical risk)\n",
        "- Features of external suppliers shipping to it (volume)\n",
        "\n",
        "This is why even simple initial features can produce powerful representations - the graph structure provides the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "create_node_features"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NODE FEATURES: Initial Representations for Each Node\n",
        "# =============================================================================\n",
        "# Each node needs an initial feature vector. The GNN will transform these\n",
        "# through message passing, but we need to start with meaningful attributes.\n",
        "#\n",
        "# Feature Engineering Choices:\n",
        "# - Continuous features (scores) are normalized to [0, 1]\n",
        "# - Categorical features use one-hot encoding\n",
        "# - Different node types can have different feature dimensions\n",
        "#   (the model will project them to a common hidden dimension)\n",
        "#\n",
        "# Normalization to [0,1] prevents features with large magnitudes from dominating gradient updates\n",
        "# =============================================================================\n",
        "\n",
        "def create_node_features(vendors_df, materials_df, regions_df, trade_data_df, mappings):\n",
        "    \"\"\"\n",
        "    Create feature tensors for each node type.\n",
        "    \n",
        "    Feature matrices have shape [num_nodes, num_features].\n",
        "    Each row is one node's feature vector.\n",
        "    \"\"\"\n",
        "    num_vendors = len(mappings['vendor'])\n",
        "    num_regions = len(mappings['region'])\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # VENDOR FEATURES: [financial_health, tier_normalized, one-hot_country...]\n",
        "    # -------------------------------------------------------------------------\n",
        "    # - Financial health (0-1): Probability of default or financial distress\n",
        "    # - Tier normalized (0-1): Tier 1 = 0.33, Tier 2 = 0.67, Tier 3 = 1.0\n",
        "    # - Country one-hot: Binary indicator for each region (captures geography)\n",
        "    vendor_features = torch.zeros(num_vendors, 2 + num_regions)\n",
        "    for _, row in vendors_df.iterrows():\n",
        "        idx = mappings['vendor'][row['VENDOR_ID']]\n",
        "        vendor_features[idx, 0] = row['FINANCIAL_HEALTH_SCORE']\n",
        "        vendor_features[idx, 1] = row['TIER'] / 3.0  # Normalize tier to [0, 1]\n",
        "        if row['COUNTRY_CODE'] in mappings['region']:\n",
        "            region_idx = mappings['region'][row['COUNTRY_CODE']]\n",
        "            vendor_features[idx, 2 + region_idx] = 1.0  # One-hot encoding\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # MATERIAL FEATURES: [criticality, material_group_one_hot]\n",
        "    # -------------------------------------------------------------------------\n",
        "    # - Criticality score: How essential is this material? (0 = commodity, 1 = critical)\n",
        "    # - Material group: RAW (raw materials), SEMI (semi-finished), FIN (finished goods)\n",
        "    num_materials = len(mappings['material'])\n",
        "    material_features = torch.zeros(num_materials, 4)\n",
        "    group_map = {'RAW': 0, 'SEMI': 1, 'FIN': 2}\n",
        "    for _, row in materials_df.iterrows():\n",
        "        idx = mappings['material'][row['MATERIAL_ID']]\n",
        "        material_features[idx, 0] = row['CRITICALITY_SCORE']\n",
        "        material_features[idx, 1 + group_map.get(row['MATERIAL_GROUP'], 0)] = 1.0\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # REGION FEATURES: [base_risk, geopolitical, natural_disaster, infrastructure]\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Multi-dimensional risk profile for each country/region\n",
        "    region_features = torch.zeros(num_regions, 4)\n",
        "    for _, row in regions_df.iterrows():\n",
        "        if row['REGION_CODE'] in mappings['region']:\n",
        "            idx = mappings['region'][row['REGION_CODE']]\n",
        "            region_features[idx, 0] = row['BASE_RISK_SCORE']       # Overall risk\n",
        "            region_features[idx, 1] = row['GEOPOLITICAL_RISK']     # Political stability\n",
        "            region_features[idx, 2] = row['NATURAL_DISASTER_RISK'] # Climate/earthquake\n",
        "            region_features[idx, 3] = row['INFRASTRUCTURE_SCORE']  # Logistics quality\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # EXTERNAL SUPPLIER FEATURES: [volume_share, shipment_frequency]\n",
        "    # -------------------------------------------------------------------------\n",
        "    # These are discovered from trade data, so we have less info than internal vendors\n",
        "    num_external = len(mappings['external'])\n",
        "    external_features = torch.zeros(num_external, 2)\n",
        "    shipper_stats = trade_data_df.groupby('SHIPPER_NAME').agg({\n",
        "        'WEIGHT_KG': 'sum',    # Total volume shipped\n",
        "        'BOL_ID': 'count'      # Number of shipments\n",
        "    }).reset_index()\n",
        "    max_volume = shipper_stats['WEIGHT_KG'].max() if len(shipper_stats) > 0 else 1\n",
        "    \n",
        "    for _, row in shipper_stats.iterrows():\n",
        "        if row['SHIPPER_NAME'] in mappings['external']:\n",
        "            idx = mappings['external'][row['SHIPPER_NAME']]\n",
        "            # Normalized volume share (0-1)\n",
        "            external_features[idx, 0] = row['WEIGHT_KG'] / max_volume if max_volume > 0 else 0\n",
        "            # Shipment frequency relative to total\n",
        "            external_features[idx, 1] = row['BOL_ID'] / len(trade_data_df)\n",
        "    \n",
        "    return {\n",
        "        'vendor': vendor_features, \n",
        "        'material': material_features, \n",
        "        'region': region_features, \n",
        "        'external': external_features\n",
        "    }\n",
        "\n",
        "node_features = create_node_features(vendors_df, materials_df, regions_df, trade_data_df, mappings)\n",
        "\n",
        "print(\"Node features created:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  {'Node Type':<12s}  {'Nodes':>6s}  {'Features':>8s}  Description\")\n",
        "print(\"-\" * 50)\n",
        "for node_type, features in node_features.items():\n",
        "    desc = {\n",
        "        'vendor': 'financial + tier + region one-hot',\n",
        "        'material': 'criticality + group one-hot',\n",
        "        'region': 'multi-risk profile',\n",
        "        'external': 'volume + frequency'\n",
        "    }\n",
        "    print(f\"  {node_type:<12s}  {features.shape[0]:>6d}  {features.shape[1]:>8d}  {desc[node_type]}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "create_edge_indices"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EDGE INDICES: Defining Graph Connectivity\n",
        "# =============================================================================\n",
        "# Edges are stored in COO (Coordinate) format as a 2xN tensor:\n",
        "#   edge_index[0] = source node indices\n",
        "#   edge_index[1] = target node indices\n",
        "#\n",
        "# For heterogeneous graphs, edges are keyed by (src_type, relation, dst_type)\n",
        "# tuples, e.g., ('vendor', 'supplies', 'material')\n",
        "# =============================================================================\n",
        "\n",
        "def create_edge_indices(purchase_orders_df, bom_df, trade_data_df, vendors_df, mappings):\n",
        "    \"\"\"\n",
        "    Build edge index tensors for each relationship type.\n",
        "    \n",
        "    Edge indices are 2xN tensors where:\n",
        "        edge_index[0, i] = source node index for edge i\n",
        "        edge_index[1, i] = destination node index for edge i\n",
        "    \"\"\"\n",
        "    edges = {}\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # SUPPLIES edges: Vendor --> Material\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Created from purchase orders: if vendor V supplied material M, add edge V->M\n",
        "    # This captures the direct Tier-1 supply relationships\n",
        "    supplies_src, supplies_dst = [], []\n",
        "    for _, row in purchase_orders_df.iterrows():\n",
        "        if row['VENDOR_ID'] in mappings['vendor'] and row['MATERIAL_ID'] in mappings['material']:\n",
        "            supplies_src.append(mappings['vendor'][row['VENDOR_ID']])\n",
        "            supplies_dst.append(mappings['material'][row['MATERIAL_ID']])\n",
        "    edges[('vendor', 'supplies', 'material')] = torch.tensor([supplies_src, supplies_dst], dtype=torch.long)\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # COMPONENT_OF edges: Material --> Material (child -> parent)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Bill of Materials (BOM) hierarchy: raw materials -> semi-finished -> finished\n",
        "    # This allows risk to propagate up the BOM tree\n",
        "    component_src, component_dst = [], []\n",
        "    for _, row in bom_df.iterrows():\n",
        "        if row['CHILD_MATERIAL_ID'] in mappings['material'] and row['PARENT_MATERIAL_ID'] in mappings['material']:\n",
        "            component_src.append(mappings['material'][row['CHILD_MATERIAL_ID']])\n",
        "            component_dst.append(mappings['material'][row['PARENT_MATERIAL_ID']])\n",
        "    edges[('material', 'component_of', 'material')] = torch.tensor([component_src, component_dst], dtype=torch.long)\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # LOCATED_IN edges: Vendor --> Region\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Geographic relationship: links vendors to their country\n",
        "    # Enables propagation of regional risk (geopolitical, natural disaster, etc.)\n",
        "    located_src, located_dst = [], []\n",
        "    for _, row in vendors_df.iterrows():\n",
        "        if row['VENDOR_ID'] in mappings['vendor'] and row['COUNTRY_CODE'] in mappings['region']:\n",
        "            located_src.append(mappings['vendor'][row['VENDOR_ID']])\n",
        "            located_dst.append(mappings['region'][row['COUNTRY_CODE']])\n",
        "    edges[('vendor', 'located_in', 'region')] = torch.tensor([located_src, located_dst], dtype=torch.long)\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # SHIPS_TO edges: External --> Vendor\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Discovered from trade/shipping data (e.g., import records, BOL data)\n",
        "    # This reveals hidden Tier-2+ suppliers that aren't in master vendor data\n",
        "    # Key insight: if external supplier E ships to vendor V, E is likely V's supplier\n",
        "    # These edges become our training signal - we learn to predict them, then find missing ones\n",
        "    ships_src, ships_dst = [], []\n",
        "    vendor_name_to_id = {row['NAME'].upper(): row['VENDOR_ID'] for _, row in vendors_df.iterrows()}\n",
        "    for _, row in trade_data_df.iterrows():\n",
        "        shipper = row['SHIPPER_NAME']\n",
        "        consignee = row['CONSIGNEE_NAME'].upper()\n",
        "        if shipper in mappings['external'] and consignee in vendor_name_to_id:\n",
        "            vendor_id = vendor_name_to_id[consignee]\n",
        "            if vendor_id in mappings['vendor']:\n",
        "                ships_src.append(mappings['external'][shipper])\n",
        "                ships_dst.append(mappings['vendor'][vendor_id])\n",
        "    edges[('external', 'ships_to', 'vendor')] = torch.tensor([ships_src, ships_dst], dtype=torch.long)\n",
        "    \n",
        "    return edges\n",
        "\n",
        "edge_indices = create_edge_indices(purchase_orders_df, bom_df, trade_data_df, vendors_df, mappings)\n",
        "\n",
        "print(\"Edge indices created:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  {'Edge Type':<45s}  {'Edges':>8s}\")\n",
        "print(\"-\" * 60)\n",
        "for edge_type, edge_index in edge_indices.items():\n",
        "    edge_str = f\"{edge_type[0]} --[{edge_type[1]}]--> {edge_type[2]}\"\n",
        "    print(f\"  {edge_str:<45s}  {edge_index.shape[1]:>8d}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  {'TOTAL EDGES':<45s}  {sum(e.shape[1] for e in edge_indices.values()):>8d}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "build_hetero_graph"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BUILD HETERODATA OBJECT\n",
        "# =============================================================================\n",
        "# HeteroData is PyG's container for heterogeneous graphs. It stores:\n",
        "#   - data['node_type'].x = node feature matrix\n",
        "#   - data[('src', 'relation', 'dst')].edge_index = edge connectivity\n",
        "#\n",
        "# ToUndirected() adds reverse edges for every edge type:\n",
        "#   e.g., ('vendor', 'supplies', 'material') â†’ ('material', 'rev_supplies', 'vendor')\n",
        "#\n",
        "# Why undirected? In GNNs, message passing happens along edges. If we only have\n",
        "# Aâ†’B edges, information can only flow from A to B. By adding Bâ†’A, we enable\n",
        "# bidirectional information flow, which is critical for learning representations\n",
        "# that capture the full graph context.\n",
        "# =============================================================================\n",
        "\n",
        "def build_hetero_graph(node_features, edge_indices):\n",
        "    \"\"\"\n",
        "    Assemble the HeteroData object from features and edges.\n",
        "    \n",
        "    This is the core data structure that gets fed to the GNN.\n",
        "    \"\"\"\n",
        "    data = HeteroData()\n",
        "    \n",
        "    # Add node features (each node type has its own feature matrix)\n",
        "    data['vendor'].x = node_features['vendor']\n",
        "    data['material'].x = node_features['material']\n",
        "    data['region'].x = node_features['region']\n",
        "    data['external'].x = node_features['external']\n",
        "    \n",
        "    # Add edge indices (only if edges exist)\n",
        "    for edge_type, edge_index in edge_indices.items():\n",
        "        if edge_index.shape[1] > 0:\n",
        "            data[edge_type].edge_index = edge_index\n",
        "    \n",
        "    return data\n",
        "\n",
        "data = build_hetero_graph(node_features, edge_indices)\n",
        "\n",
        "# Add reverse edges for bidirectional message passing\n",
        "# This transforms directed edges Aâ†’B into undirected Aâ†”B\n",
        "data = ToUndirected()(data)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HETEROGENEOUS GRAPH CONSTRUCTED\")\n",
        "print(\"=\" * 60)\n",
        "print(data)\n",
        "print(\"\\nNote: 'rev_*' edge types are reverse edges added by ToUndirected()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "visualize_graph_structure"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GRAPH STRUCTURE VISUALIZATION\n",
        "# =============================================================================\n",
        "# Visualize the graph statistics to understand the network structure\n",
        "# Imbalanced node/edge counts can cause learning issues - catch problems early before training\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# Plot 1: Node counts by type\n",
        "node_counts = {k: len(v) for k, v in mappings.items()}\n",
        "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
        "axes[0].bar(node_counts.keys(), node_counts.values(), color=colors)\n",
        "axes[0].set_title('Node Counts by Type', fontweight='bold')\n",
        "axes[0].set_ylabel('Count')\n",
        "for i, (k, v) in enumerate(node_counts.items()):\n",
        "    axes[0].text(i, v + max(node_counts.values())*0.02, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 2: Edge counts by type\n",
        "edge_counts = {f\"{e[0][:3]}â†’{e[2][:3]}\\n({e[1][:6]})\": idx.shape[1] \n",
        "               for e, idx in edge_indices.items()}\n",
        "axes[1].bar(range(len(edge_counts)), edge_counts.values(), color='#34495e')\n",
        "axes[1].set_xticks(range(len(edge_counts)))\n",
        "axes[1].set_xticklabels(edge_counts.keys(), fontsize=8)\n",
        "axes[1].set_title('Edge Counts by Relationship', fontweight='bold')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "# Plot 3: Feature dimensions by node type\n",
        "feat_dims = {k: v.shape[1] for k, v in node_features.items()}\n",
        "axes[2].barh(list(feat_dims.keys()), list(feat_dims.values()), color=colors)\n",
        "axes[2].set_title('Feature Dimensions by Node Type', fontweight='bold')\n",
        "axes[2].set_xlabel('Number of Features')\n",
        "for i, (k, v) in enumerate(feat_dims.items()):\n",
        "    axes[2].text(v + 0.5, i, str(v), va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/graph_structure.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Graph Density Analysis:\")\n",
        "total_nodes = sum(node_counts.values())\n",
        "total_edges = sum(e.shape[1] for e in edge_indices.values())\n",
        "print(f\"   Total nodes: {total_nodes}\")\n",
        "print(f\"   Total edges: {total_edges}\")\n",
        "print(f\"   Average degree: {2 * total_edges / total_nodes:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "define_gnn_model_header"
      },
      "source": [
        "## 4. Define GNN Model\n",
        "\n",
        "### GraphSAGE: How It Works\n",
        "\n",
        "**GraphSAGE** (Graph SAmple and aggreGatE) learns node representations by aggregating features from neighboring nodes. Unlike traditional methods that require the full graph at inference time, GraphSAGE learns an **aggregation function** that can generalize to unseen nodes.\n",
        "\n",
        "**Message Passing (one layer):**\n",
        "```\n",
        "h_v^(l+1) = Ïƒ( W Â· AGGREGATE({h_u^(l) : u âˆˆ N(v)}) + B Â· h_v^(l) )\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `h_v^(l)` = embedding of node v at layer l\n",
        "- `N(v)` = neighbors of node v  \n",
        "- `AGGREGATE` = mean/sum/max of neighbor embeddings\n",
        "- `W, B` = learnable weight matrices\n",
        "- `Ïƒ` = activation function (ReLU)\n",
        "\n",
        "**Why 2 Layers?**\n",
        "- Layer 1: Each node sees its immediate neighbors (1-hop)\n",
        "- Layer 2: Each node sees neighbors of neighbors (2-hop)\n",
        "- This captures Tier-1 AND Tier-2 relationships in one forward pass\n",
        "\n",
        "**Heterogeneous Extension:**\n",
        "For each edge type, we have separate weight matrices. This allows the model to learn that \"SUPPLIES\" relationships matter differently than \"LOCATED_IN\" relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "message_passing_explainer"
      },
      "source": [
        "### Deep Dive: How Message Passing Actually Works\n",
        "\n",
        "This is the core of what makes GNNs powerful. Let's trace through what happens to a single vendor node:\n",
        "\n",
        "**Layer 1 (1-hop):**\n",
        "```\n",
        "Vendor_A receives messages from:\n",
        "  â”œâ”€â”€ Material_X (via SUPPLIES edge) â†’ \"I'm a critical component\"\n",
        "  â”œâ”€â”€ Material_Y (via SUPPLIES edge) â†’ \"I'm a commodity\"  \n",
        "  â”œâ”€â”€ Region_CN (via LOCATED_IN edge) â†’ \"I have high geopolitical risk\"\n",
        "  â””â”€â”€ External_Z (via SHIPS_TO edge) â†’ \"I ship large volumes\"\n",
        "\n",
        "Vendor_A aggregates these: new_embedding = MEAN(all neighbor embeddings)\n",
        "Then transforms: final = ReLU(W Ã— new_embedding + b)\n",
        "```\n",
        "\n",
        "**Layer 2 (2-hop):**\n",
        "```\n",
        "Now Vendor_A's neighbors have ALREADY incorporated THEIR neighbors:\n",
        "  â”œâ”€â”€ Material_X now knows about OTHER vendors that supply it\n",
        "  â”œâ”€â”€ Region_CN now knows about OTHER vendors in the same region\n",
        "  â””â”€â”€ External_Z now knows about OTHER vendors it ships to\n",
        "\n",
        "When Vendor_A aggregates again, it indirectly learns about:\n",
        "  - Competitors (other vendors supplying same materials)\n",
        "  - Geographic peers (other vendors in same region)  \n",
        "  - Shared suppliers (other vendors using same external supplier)\n",
        "```\n",
        "\n",
        "**This is how hidden Tier-2 dependencies emerge**: Two vendors that share an external supplier will have similar embeddings even if they have no direct connection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "define_hetero_graphsage_model"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HETEROGENEOUS GRAPHSAGE MODEL\n",
        "# =============================================================================\n",
        "# This model has three main components:\n",
        "#   1. Input projection: Map each node type's features to a common dimension\n",
        "#   2. Message passing layers: Aggregate information from neighbors\n",
        "#   3. Task heads: Predict risk scores or link probabilities\n",
        "# =============================================================================\n",
        "\n",
        "class HeteroGraphSAGE(nn.Module):\n",
        "    \"\"\"\n",
        "    Heterogeneous GraphSAGE model for supply chain risk analysis.\n",
        "    \n",
        "    Architecture:\n",
        "        Input Features (varying dim) \n",
        "            â†’ Linear projection (hidden_channels)\n",
        "            â†’ ReLU\n",
        "            â†’ GraphSAGE Conv Layer 1 (hidden_channels)\n",
        "            â†’ ReLU + Dropout\n",
        "            â†’ GraphSAGE Conv Layer 2 (out_channels)\n",
        "            â†’ Task-specific heads\n",
        "    \n",
        "    The model learns:\n",
        "        - Node embeddings that capture graph structure\n",
        "        - How risk propagates across different relationship types\n",
        "        - Which external suppliers are likely connected to which vendors\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels_dict, hidden_channels=64, out_channels=32):\n",
        "        super().__init__()\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        \n",
        "        # ---------------------------------------------------------------------\n",
        "        # INPUT PROJECTION LAYERS\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Each node type has different input feature dimensions.\n",
        "        # We project all of them to a common hidden dimension.\n",
        "        # This is like creating a shared \"embedding space\" for all node types.\n",
        "        self.lin_dict = nn.ModuleDict()\n",
        "        for node_type, in_channels in in_channels_dict.items():\n",
        "            self.lin_dict[node_type] = Linear(in_channels, hidden_channels)\n",
        "        \n",
        "        # ---------------------------------------------------------------------\n",
        "        # FIRST GRAPH CONVOLUTION LAYER (1-hop neighborhood)\n",
        "        # ---------------------------------------------------------------------\n",
        "        # HeteroConv wraps separate convolutions for each edge type.\n",
        "        # Each edge type has its own weights, allowing the model to learn\n",
        "        # that \"supplies\" relationships differ from \"located_in\" relationships.\n",
        "        #\n",
        "        # aggr='mean': If a node receives messages from multiple edge types,\n",
        "        # we average them. Alternatives: 'sum', 'max'.\n",
        "        self.conv1 = HeteroConv({\n",
        "            # Forward edges (original relationships)\n",
        "            ('vendor', 'supplies', 'material'): SAGEConv(hidden_channels, hidden_channels),\n",
        "            ('material', 'component_of', 'material'): SAGEConv(hidden_channels, hidden_channels),\n",
        "            ('vendor', 'located_in', 'region'): SAGEConv(hidden_channels, hidden_channels),\n",
        "            ('external', 'ships_to', 'vendor'): SAGEConv(hidden_channels, hidden_channels),\n",
        "            # Reverse edges (added by ToUndirected)\n",
        "            ('material', 'rev_supplies', 'vendor'): SAGEConv(hidden_channels, hidden_channels),\n",
        "            ('material', 'rev_component_of', 'material'): SAGEConv(hidden_channels, hidden_channels),\n",
        "            ('region', 'rev_located_in', 'vendor'): SAGEConv(hidden_channels, hidden_channels),\n",
        "            ('vendor', 'rev_ships_to', 'external'): SAGEConv(hidden_channels, hidden_channels),\n",
        "        }, aggr='mean')\n",
        "        \n",
        "        # ---------------------------------------------------------------------\n",
        "        # SECOND GRAPH CONVOLUTION LAYER (2-hop neighborhood)\n",
        "        # ---------------------------------------------------------------------\n",
        "        # After this layer, each node's embedding contains information from\n",
        "        # nodes up to 2 hops away. This captures Tier-2 relationships.\n",
        "        self.conv2 = HeteroConv({\n",
        "            ('vendor', 'supplies', 'material'): SAGEConv(hidden_channels, out_channels),\n",
        "            ('material', 'component_of', 'material'): SAGEConv(hidden_channels, out_channels),\n",
        "            ('vendor', 'located_in', 'region'): SAGEConv(hidden_channels, out_channels),\n",
        "            ('external', 'ships_to', 'vendor'): SAGEConv(hidden_channels, out_channels),\n",
        "            ('material', 'rev_supplies', 'vendor'): SAGEConv(hidden_channels, out_channels),\n",
        "            ('material', 'rev_component_of', 'material'): SAGEConv(hidden_channels, out_channels),\n",
        "            ('region', 'rev_located_in', 'vendor'): SAGEConv(hidden_channels, out_channels),\n",
        "            ('vendor', 'rev_ships_to', 'external'): SAGEConv(hidden_channels, out_channels),\n",
        "        }, aggr='mean')\n",
        "        \n",
        "        # ---------------------------------------------------------------------\n",
        "        # RISK PREDICTION HEADS\n",
        "        # ---------------------------------------------------------------------\n",
        "        # Simple linear layers that map embeddings to a single risk score.\n",
        "        # Sigmoid activation bounds output to [0, 1].\n",
        "        # Note: 'region' doesn't have a risk head - we use raw region features.\n",
        "        self.risk_head = nn.ModuleDict({\n",
        "            'vendor': nn.Linear(out_channels, 1),\n",
        "            'material': nn.Linear(out_channels, 1),\n",
        "            'external': nn.Linear(out_channels, 1),\n",
        "        })\n",
        "    \n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        \"\"\"\n",
        "        Forward pass: compute node embeddings.\n",
        "        \n",
        "        Args:\n",
        "            x_dict: Dict mapping node_type -> feature tensor\n",
        "            edge_index_dict: Dict mapping edge_type -> edge indices\n",
        "            \n",
        "        Returns:\n",
        "            Dict mapping node_type -> embedding tensor\n",
        "        \"\"\"\n",
        "        # Step 1: Project all node types to hidden dimension\n",
        "        x_dict = {key: self.lin_dict[key](x) for key, x in x_dict.items()}\n",
        "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
        "        \n",
        "        # Step 2: First round of message passing (1-hop)\n",
        "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
        "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
        "        \n",
        "        # Dropout for regularization (only during training)\n",
        "        x_dict = {key: F.dropout(x, p=0.3, training=self.training) for key, x in x_dict.items()}\n",
        "        \n",
        "        # Step 3: Second round of message passing (2-hop)\n",
        "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
        "        \n",
        "        return x_dict\n",
        "    \n",
        "    def predict_risk(self, x_dict):\n",
        "        \"\"\"\n",
        "        Predict risk scores for each node.\n",
        "        \n",
        "        Returns:\n",
        "            Dict mapping node_type -> risk scores in [0, 1]\n",
        "        \"\"\"\n",
        "        risk_scores = {}\n",
        "        for node_type, head in self.risk_head.items():\n",
        "            if node_type in x_dict:\n",
        "                # Sigmoid squashes output to [0, 1] for probability interpretation\n",
        "                risk_scores[node_type] = torch.sigmoid(head(x_dict[node_type]))\n",
        "        return risk_scores\n",
        "    \n",
        "    def predict_link(self, z_src, z_dst):\n",
        "        \"\"\"\n",
        "        Predict probability of a link between source and destination nodes.\n",
        "        \n",
        "        This uses a simple dot-product decoder:\n",
        "            P(edge exists) = sigmoid(z_src Â· z_dst)\n",
        "        \n",
        "        Intuition: If two nodes have similar embeddings (high dot product),\n",
        "        they're likely connected.\n",
        "        \"\"\"\n",
        "        return torch.sigmoid((z_src * z_dst).sum(dim=-1))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MODEL INITIALIZATION\n",
        "# =============================================================================\n",
        "# Hyperparameters:\n",
        "#   hidden_channels=64: Dimension of intermediate representations\n",
        "#   out_channels=32: Dimension of final embeddings (used for link prediction)\n",
        "# These dimensions balance expressiveness vs overfitting for typical supply chain graphs (<10K nodes)\n",
        "# =============================================================================\n",
        "\n",
        "in_channels_dict = {key: feat.shape[1] for key, feat in node_features.items()}\n",
        "model = HeteroGraphSAGE(in_channels_dict, hidden_channels=64, out_channels=32)\n",
        "\n",
        "# Move to GPU if available (significant speedup for large graphs)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "# Model summary\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Hidden dimension: 64\")\n",
        "print(f\"Output embedding dimension: 32\")\n",
        "print(f\"Number of GNN layers: 2 (captures 2-hop neighborhoods)\")\n",
        "print(f\"Dropout rate: 0.3\")\n",
        "print(\"-\" * 60)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "training_header"
      },
      "source": [
        "## 5. Model Training\n",
        "\n",
        "### Training Objective: Link Prediction\n",
        "\n",
        "We train the model using **link prediction** as a self-supervised task:\n",
        "- **Positive samples**: Edges that exist (External â†’ Vendor shipments)\n",
        "- **Negative samples**: Random pairs that don't have edges\n",
        "\n",
        "**Loss Function**: Binary Cross-Entropy\n",
        "```\n",
        "L = -[yÂ·log(p) + (1-y)Â·log(1-p)]\n",
        "```\n",
        "Where y=1 for real edges, y=0 for fake edges.\n",
        "\n",
        "**Why Link Prediction?**\n",
        "1. It's self-supervised (no manual labels needed)\n",
        "2. Forces the model to learn meaningful embeddings\n",
        "3. The trained model can predict NEW links (hidden Tier-2 relationships)\n",
        "\n",
        "**Regularization**:\n",
        "- L2 regularization on embeddings prevents overfitting\n",
        "- Dropout (30%) during training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "self_supervised_training_explainer"
      },
      "source": [
        "### The Self-Supervised Training Trick\n",
        "\n",
        "A common question: **\"How can we train this model without labeled risk data?\"**\n",
        "\n",
        "The answer is **link prediction as a proxy task**. Here's the insight:\n",
        "\n",
        "1. We have some SHIPS_TO edges from trade data (external supplier â†’ vendor)\n",
        "2. We train the model to predict: \"Given embeddings of External_A and Vendor_B, are they connected?\"\n",
        "3. The model learns embeddings where **connected nodes are similar** (high dot product)\n",
        "\n",
        "**But here's the magic**: To predict links well, the model MUST learn meaningful representations. It has to understand:\n",
        "- What makes a vendor likely to receive shipments from a particular external supplier?\n",
        "- What patterns in the graph indicate a supply relationship?\n",
        "\n",
        "Once trained, these embeddings capture the **latent structure of the supply chain**. We can then:\n",
        "- Use embeddings to predict risk (via the risk head)\n",
        "- Find missing links (hidden dependencies)\n",
        "- Identify similar nodes (potential alternative suppliers)\n",
        "\n",
        "**The negative sampling is critical**: If we only showed positive examples (real edges), the model would learn to predict 1.0 for everything. By showing random non-edges as negatives, we force it to learn discriminative features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "train_model"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL TRAINING\n",
        "# =============================================================================\n",
        "# We use link prediction as the training objective. The model learns to\n",
        "# distinguish real edges (positive samples) from fake edges (negative samples).\n",
        "#\n",
        "# Training Loop:\n",
        "#   1. Forward pass: Compute embeddings for all nodes\n",
        "#   2. Sample positive edges (real SHIPS_TO relationships)\n",
        "#   3. Sample negative edges (random external-vendor pairs)\n",
        "#   4. Compute loss: BCE for positive + BCE for negative + L2 regularization\n",
        "#   5. Backward pass: Compute gradients\n",
        "#   6. Update weights with Adam optimizer\n",
        "# =============================================================================\n",
        "\n",
        "def train_model(model, data, epochs=100, lr=0.01):\n",
        "    \"\"\"\n",
        "    Train the GNN model using link prediction.\n",
        "    \n",
        "    Args:\n",
        "        model: HeteroGraphSAGE model\n",
        "        data: HeteroData graph object\n",
        "        epochs: Number of training iterations\n",
        "        lr: Learning rate for Adam optimizer\n",
        "        \n",
        "    Returns:\n",
        "        List of loss values per epoch (for plotting)\n",
        "    \"\"\"\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    model.train()  # Enable dropout and training mode\n",
        "    \n",
        "    # We train on the SHIPS_TO edges (external supplier â†’ vendor)\n",
        "    # These represent the relationships we want to predict for hidden suppliers\n",
        "    edge_type = ('external', 'ships_to', 'vendor')\n",
        "    pos_edge_index = data[edge_type].edge_index if edge_type in data.edge_index_dict else None\n",
        "    \n",
        "    losses = []\n",
        "    pos_losses = []\n",
        "    neg_losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass: compute node embeddings\n",
        "        z_dict = model(data.x_dict, data.edge_index_dict)\n",
        "        \n",
        "        loss = 0\n",
        "        pos_loss_val = 0\n",
        "        neg_loss_val = 0\n",
        "        \n",
        "        if pos_edge_index is not None and pos_edge_index.shape[1] > 0:\n",
        "            # -----------------------------------------------------------------\n",
        "            # POSITIVE SAMPLES: Real edges that exist\n",
        "            # -----------------------------------------------------------------\n",
        "            # For each real edge (external[i] â†’ vendor[j]), we want\n",
        "            # the model to predict high probability (close to 1)\n",
        "            z_src = z_dict['external'][pos_edge_index[0]]\n",
        "            z_dst = z_dict['vendor'][pos_edge_index[1]]\n",
        "            pos_pred = model.predict_link(z_src, z_dst)\n",
        "            pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred))\n",
        "            pos_loss_val = pos_loss.item()\n",
        "            \n",
        "            # -----------------------------------------------------------------\n",
        "            # NEGATIVE SAMPLES: Random pairs (likely non-edges)\n",
        "            # -----------------------------------------------------------------\n",
        "            # For random pairs, we want the model to predict low probability\n",
        "            # We sample the same number of negatives as positives (balanced)\n",
        "            # Without negatives, the model would predict 1.0 for everything - negatives teach it to discriminate\n",
        "            num_neg = pos_edge_index.shape[1]\n",
        "            neg_src = torch.randint(0, z_dict['external'].shape[0], (num_neg,), device=device)\n",
        "            neg_dst = torch.randint(0, z_dict['vendor'].shape[0], (num_neg,), device=device)\n",
        "            neg_pred = model.predict_link(z_dict['external'][neg_src], z_dict['vendor'][neg_dst])\n",
        "            neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
        "            neg_loss_val = neg_loss.item()\n",
        "            \n",
        "            loss += pos_loss + neg_loss\n",
        "        \n",
        "        # -----------------------------------------------------------------\n",
        "        # L2 REGULARIZATION: Prevent embeddings from exploding\n",
        "        # -----------------------------------------------------------------\n",
        "        # Small penalty on the magnitude of all embeddings\n",
        "        # Coefficient 0.001 is typical (not too strong)\n",
        "        for node_type, z in z_dict.items():\n",
        "            loss += 0.001 * torch.norm(z, p=2)\n",
        "        \n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track losses for visualization\n",
        "        losses.append(loss.item())\n",
        "        pos_losses.append(pos_loss_val)\n",
        "        neg_losses.append(neg_loss_val)\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{epochs} | Total Loss: {loss.item():.4f} | \"\n",
        "                  f\"Pos: {pos_loss_val:.4f} | Neg: {neg_loss_val:.4f}\")\n",
        "    \n",
        "    return losses, pos_losses, neg_losses\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING GNN MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Epochs: 100 | Learning Rate: 0.01 | Optimizer: Adam\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "losses, pos_losses, neg_losses = train_model(model, data, epochs=100, lr=0.01)\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"âœ… Training complete!\")\n",
        "print(f\"   Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"   Loss reduction: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "training_diagnostics_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING DIAGNOSTICS VISUALIZATION\n",
        "# =============================================================================\n",
        "# Visualize training progress to check for convergence and potential issues\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot 1: Total Loss Curve\n",
        "axes[0].plot(losses, color='#2c3e50', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Total Loss')\n",
        "axes[0].set_title('Training Loss Curve', fontweight='bold')\n",
        "axes[0].axhline(y=losses[-1], color='#e74c3c', linestyle='--', alpha=0.7, label=f'Final: {losses[-1]:.4f}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Positive vs Negative Loss\n",
        "axes[1].plot(pos_losses, label='Positive (real edges)', color='#27ae60', linewidth=2)\n",
        "axes[1].plot(neg_losses, label='Negative (fake edges)', color='#e74c3c', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('BCE Loss')\n",
        "axes[1].set_title('Link Prediction Loss Components', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Loss ratio (should converge to ~1 for balanced learning)\n",
        "# Ratio >> 1 means model ignores negatives (overpredicts); << 1 means ignores positives (underpredicts)\n",
        "loss_ratio = [p / (n + 1e-8) for p, n in zip(pos_losses, neg_losses)]\n",
        "axes[2].plot(loss_ratio, color='#9b59b6', linewidth=2)\n",
        "axes[2].axhline(y=1.0, color='#34495e', linestyle='--', alpha=0.7, label='Balanced (1.0)')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Pos/Neg Loss Ratio')\n",
        "axes[2].set_title('Loss Balance', fontweight='bold')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_ylim(0, 3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/training_diagnostics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Training quality assessment\n",
        "print(\"\\nðŸ“Š Training Quality Assessment:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check convergence\n",
        "if losses[-1] < losses[0] * 0.5:\n",
        "    print(\"âœ… Good convergence: Loss reduced by >50%\")\n",
        "else:\n",
        "    print(\"âš ï¸  Limited convergence: Consider more epochs or lower LR\")\n",
        "\n",
        "# Check balance\n",
        "final_ratio = pos_losses[-1] / (neg_losses[-1] + 1e-8)\n",
        "if 0.5 < final_ratio < 2.0:\n",
        "    print(\"âœ… Balanced learning: Pos/Neg ratio near 1.0\")\n",
        "else:\n",
        "    print(f\"âš ï¸  Imbalanced: Pos/Neg ratio = {final_ratio:.2f}\")\n",
        "\n",
        "# Check for underfitting/overfitting\n",
        "if losses[-1] < 0.5:\n",
        "    print(\"âœ… Low final loss: Model learned the task\")\n",
        "else:\n",
        "    print(\"âš ï¸  High final loss: May need more capacity or data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "compute_risk_scores_header"
      },
      "source": [
        "## 6. Compute Risk Scores\n",
        "\n",
        "### Risk Scoring Approach\n",
        "\n",
        "The GNN learns embeddings that capture each node's position in the supply chain network. We convert these embeddings to risk scores using:\n",
        "\n",
        "1. **Learned Risk Head**: A linear layer trained to predict risk from embeddings\n",
        "2. **Regional Risk Blending**: Vendor risk is adjusted based on their region's risk profile\n",
        "\n",
        "**Risk Score Formula for Vendors:**\n",
        "```\n",
        "final_risk = 0.6 Ã— learned_risk + 0.4 Ã— regional_risk\n",
        "```\n",
        "\n",
        "**Regional Risk Calculation:**\n",
        "```\n",
        "regional_risk = 0.3Ã—base + 0.4Ã—geopolitical + 0.2Ã—natural_disaster + 0.1Ã—(1-infrastructure)\n",
        "```\n",
        "\n",
        "The weights reflect that geopolitical risk (tariffs, sanctions, instability) tends to be more impactful than infrastructure quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "inference_mode_explainer"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RISK SCORE COMPUTATION\n",
        "# =============================================================================\n",
        "# We combine learned embeddings with domain knowledge (regional risk factors)\n",
        "# to produce interpretable risk scores for each node.\n",
        "# =============================================================================\n",
        "\n",
        "def compute_risk_scores(model, data, regions_df, mappings):\n",
        "    \"\"\"\n",
        "    Compute risk scores for all nodes with region risk propagation.\n",
        "    \n",
        "    The model's risk head produces a raw learned risk score.\n",
        "    We then blend vendor risk with their region's risk profile.\n",
        "    \n",
        "    Returns:\n",
        "        risk_scores: Dict mapping node_type -> risk tensor [0, 1]\n",
        "        z_dict: Dict mapping node_type -> embedding tensor\n",
        "    \"\"\"\n",
        "    model.eval()  # Disable dropout for inference\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Forward pass to get embeddings\n",
        "        z_dict = model(data.x_dict, data.edge_index_dict)\n",
        "        # Apply risk prediction heads\n",
        "        risk_scores = model.predict_risk(z_dict)\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # REGIONAL RISK CALCULATION\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Weighted combination of region risk factors\n",
        "    # Weights based on typical supply chain risk impact:\n",
        "    #   - Geopolitical (40%): Tariffs, sanctions, political instability\n",
        "    #   - Base risk (30%): Overall country risk assessment\n",
        "    #   - Natural disasters (20%): Earthquakes, typhoons, flooding\n",
        "    #   - Infrastructure (10%): Ports, roads, logistics quality (inverted)\n",
        "    region_risks = {}\n",
        "    for _, row in regions_df.iterrows():\n",
        "        if row['REGION_CODE'] in mappings['region']:\n",
        "            region_risk = (\n",
        "                row['BASE_RISK_SCORE'] * 0.3 + \n",
        "                row['GEOPOLITICAL_RISK'] * 0.4 +\n",
        "                row['NATURAL_DISASTER_RISK'] * 0.2 + \n",
        "                (1 - row['INFRASTRUCTURE_SCORE']) * 0.1  # Invert: low infrastructure = high risk\n",
        "            )\n",
        "            region_risks[mappings['region'][row['REGION_CODE']]] = region_risk\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # BLEND VENDOR RISK WITH REGIONAL RISK\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Final vendor risk = 60% learned + 40% regional\n",
        "    # This ensures geographic factors are incorporated\n",
        "    # Pure ML risk misses domain knowledge; blending adds interpretability and captures known risk factors\n",
        "    if ('vendor', 'located_in', 'region') in data.edge_index_dict:\n",
        "        edge_index = data[('vendor', 'located_in', 'region')].edge_index\n",
        "        vendor_risk = risk_scores['vendor'].squeeze().cpu().numpy()\n",
        "        \n",
        "        for i in range(edge_index.shape[1]):\n",
        "            vendor_idx = edge_index[0, i].item()\n",
        "            region_idx = edge_index[1, i].item()\n",
        "            if region_idx in region_risks:\n",
        "                # Weighted blend of learned risk and regional risk\n",
        "                vendor_risk[vendor_idx] = (\n",
        "                    vendor_risk[vendor_idx] * 0.6 + \n",
        "                    region_risks[region_idx] * 0.4\n",
        "                )\n",
        "        \n",
        "        risk_scores['vendor'] = torch.tensor(vendor_risk).unsqueeze(1)\n",
        "    \n",
        "    return risk_scores, z_dict\n",
        "\n",
        "risk_scores, embeddings = compute_risk_scores(model, data, regions_df, mappings)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RISK SCORES COMPUTED\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Node Type':<15s} {'Min':>8s} {'Max':>8s} {'Mean':>8s} {'Std':>8s}\")\n",
        "print(\"-\" * 60)\n",
        "for node_type, scores in risk_scores.items():\n",
        "    scores_np = scores.squeeze().cpu().numpy()\n",
        "    print(f\"{node_type:<15s} {scores_np.min():>8.3f} {scores_np.max():>8.3f} \"\n",
        "          f\"{scores_np.mean():>8.3f} {scores_np.std():>8.3f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "compute_risk_scores"
      },
      "source": [
        "### Inference Mode: From Embeddings to Risk Scores\n",
        "\n",
        "Training and inference use the model differently:\n",
        "\n",
        "| Aspect | Training | Inference |\n",
        "|--------|----------|-----------|\n",
        "| Mode | `model.train()` | `model.eval()` |\n",
        "| Dropout | Active (30% of neurons randomly zeroed) | Disabled (all neurons active) |\n",
        "| Gradients | Computed for backprop | Disabled (`torch.no_grad()`) |\n",
        "| Output | Loss value | Embeddings + predictions |\n",
        "\n",
        "**Why disable dropout at inference?** During training, dropout provides regularization by forcing the model to not rely on any single neuron. At inference, we want the full model capacity for the best predictions. The dropout rate (0.3) means we effectively trained an ensemble - at inference we use the averaged model.\n",
        "\n",
        "**The risk blending formula** deserves attention:\n",
        "```\n",
        "final_risk = 0.6 Ã— learned_risk + 0.4 Ã— regional_risk\n",
        "```\n",
        "\n",
        "This isn't arbitrary. We weight learned risk higher (0.6) because it captures network effects the regional risk can't see. But we keep 40% regional risk because:\n",
        "1. It provides interpretability (\"high risk due to geopolitical factors\")\n",
        "2. It acts as a prior when the model has limited data for a vendor\n",
        "3. It captures domain knowledge that may not be in the training signal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "risk_score_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RISK SCORE DISTRIBUTION VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Color scheme for risk levels\n",
        "risk_colors = {'LOW': '#27ae60', 'MEDIUM': '#f39c12', 'HIGH': '#e67e22', 'CRITICAL': '#e74c3c'}\n",
        "\n",
        "# Plot 1: Risk score distributions by node type\n",
        "ax = axes[0, 0]\n",
        "for i, (node_type, scores) in enumerate(risk_scores.items()):\n",
        "    scores_np = scores.squeeze().cpu().numpy()\n",
        "    ax.hist(scores_np, bins=20, alpha=0.6, label=node_type, density=True)\n",
        "ax.set_xlabel('Risk Score')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Risk Score Distribution by Node Type', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.axvline(x=0.25, color='green', linestyle='--', alpha=0.5, label='LOW/MEDIUM')\n",
        "ax.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='MEDIUM/HIGH')\n",
        "ax.axvline(x=0.75, color='red', linestyle='--', alpha=0.5, label='HIGH/CRITICAL')\n",
        "\n",
        "# Plot 2: Risk category counts\n",
        "ax = axes[0, 1]\n",
        "def categorize_risk(score):\n",
        "    if score < 0.25: return 'LOW'\n",
        "    elif score < 0.5: return 'MEDIUM'\n",
        "    elif score < 0.75: return 'HIGH'\n",
        "    else: return 'CRITICAL'\n",
        "\n",
        "category_counts = defaultdict(lambda: defaultdict(int))\n",
        "for node_type, scores in risk_scores.items():\n",
        "    for score in scores.squeeze().cpu().numpy():\n",
        "        cat = categorize_risk(score)\n",
        "        category_counts[node_type][cat] += 1\n",
        "\n",
        "x = np.arange(len(risk_scores))\n",
        "width = 0.2\n",
        "categories = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']\n",
        "for i, cat in enumerate(categories):\n",
        "    values = [category_counts[nt][cat] for nt in risk_scores.keys()]\n",
        "    ax.bar(x + i*width, values, width, label=cat, color=risk_colors[cat])\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(risk_scores.keys())\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Risk Categories by Node Type', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 3: Embedding visualization (PCA)\n",
        "# Well-separated clusters in embedding space indicate the model learned meaningful node distinctions\n",
        "ax = axes[1, 0]\n",
        "# Combine all embeddings for visualization\n",
        "all_emb = []\n",
        "all_labels = []\n",
        "all_risks = []\n",
        "for node_type, emb in embeddings.items():\n",
        "    emb_np = emb.cpu().numpy()\n",
        "    all_emb.append(emb_np)\n",
        "    all_labels.extend([node_type] * len(emb_np))\n",
        "    if node_type in risk_scores:\n",
        "        all_risks.extend(risk_scores[node_type].squeeze().cpu().numpy().tolist())\n",
        "    else:\n",
        "        all_risks.extend([0.5] * len(emb_np))\n",
        "\n",
        "all_emb = np.vstack(all_emb)\n",
        "pca = PCA(n_components=2)\n",
        "emb_2d = pca.fit_transform(all_emb)\n",
        "\n",
        "# Plot by node type with risk as color intensity\n",
        "node_type_colors = {'vendor': '#2ecc71', 'material': '#3498db', 'region': '#e74c3c', 'external': '#9b59b6'}\n",
        "for node_type in set(all_labels):\n",
        "    mask = [l == node_type for l in all_labels]\n",
        "    ax.scatter(emb_2d[mask, 0], emb_2d[mask, 1], c=node_type_colors.get(node_type, 'gray'), \n",
        "               alpha=0.6, label=node_type, s=30)\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)')\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)')\n",
        "ax.set_title('Node Embeddings (PCA)', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 4: High-risk nodes\n",
        "ax = axes[1, 1]\n",
        "high_risk_data = []\n",
        "for node_type, scores in risk_scores.items():\n",
        "    scores_np = scores.squeeze().cpu().numpy()\n",
        "    high_risk_count = np.sum(scores_np >= 0.5)  # HIGH or CRITICAL\n",
        "    high_risk_data.append({'type': node_type, 'high_risk': high_risk_count, \n",
        "                           'total': len(scores_np), 'pct': high_risk_count/len(scores_np)*100})\n",
        "\n",
        "df_hr = pd.DataFrame(high_risk_data)\n",
        "bars = ax.barh(df_hr['type'], df_hr['pct'], color=['#e74c3c' if p > 30 else '#f39c12' if p > 15 else '#27ae60' \n",
        "                                                    for p in df_hr['pct']])\n",
        "ax.set_xlabel('% High Risk (score â‰¥ 0.5)')\n",
        "ax.set_title('High-Risk Node Proportion', fontweight='bold')\n",
        "ax.set_xlim(0, 100)\n",
        "for i, (p, t) in enumerate(zip(df_hr['pct'], df_hr['total'])):\n",
        "    ax.text(p + 2, i, f'{p:.1f}% ({int(p*t/100)}/{t})', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/risk_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸŽ¯ Key Findings:\")\n",
        "for row in high_risk_data:\n",
        "    if row['pct'] > 20:\n",
        "        print(f\"   âš ï¸  {row['type']}: {row['pct']:.1f}% are HIGH/CRITICAL risk\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "hidden_dependencies_header"
      },
      "source": [
        "## 7. Identify Hidden Dependencies and Bottlenecks\n",
        "\n",
        "### Link Prediction for Hidden Dependencies\n",
        "\n",
        "The trained model can predict **missing edges** - supplier relationships that exist but aren't in our master data:\n",
        "\n",
        "```\n",
        "P(edge exists) = sigmoid(z_external Â· z_vendor)\n",
        "```\n",
        "\n",
        "If two nodes have similar learned embeddings (high dot product), they're likely connected even if we don't have explicit records.\n",
        "\n",
        "### Bottleneck Detection\n",
        "\n",
        "A **bottleneck** (single point of failure) is an external supplier that many vendors depend on. If this supplier fails:\n",
        "- All dependent vendors are impacted\n",
        "- Risk cascades through the network\n",
        "- Alternative suppliers may not exist\n",
        "\n",
        "**Impact Score**: `min(1.0, dependent_count / 10)`\n",
        "- 10+ dependents = maximum impact (1.0)\n",
        "- Scales linearly below that\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "link_prediction_explainer"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HIDDEN LINK PREDICTION\n",
        "# =============================================================================\n",
        "# Use the trained model to discover supplier relationships that aren't in our\n",
        "# master data. This reveals hidden Tier-2+ dependencies.\n",
        "#\n",
        "# Method: For every (external, vendor) pair, compute the link probability.\n",
        "# If probability > threshold, we predict a hidden relationship exists.\n",
        "# =============================================================================\n",
        "\n",
        "def predict_hidden_links(model, embeddings, mappings, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Predict hidden links between external suppliers and vendors.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained GNN model with predict_link method\n",
        "        embeddings: Dict of node embeddings from forward pass\n",
        "        mappings: Node ID to index mappings\n",
        "        threshold: Minimum probability to consider a link (0.3 = 30% confidence)\n",
        "        \n",
        "    Returns:\n",
        "        List of predicted links with probabilities\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predicted_links = []\n",
        "    z_external = embeddings['external']\n",
        "    z_vendor = embeddings['vendor']\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Iterate over all possible (external, vendor) pairs\n",
        "        # For large graphs, this would need batching or sampling\n",
        "        for ext_idx in range(z_external.shape[0]):\n",
        "            for ven_idx in range(z_vendor.shape[0]):\n",
        "                # Dot-product similarity â†’ probability via sigmoid\n",
        "                prob = model.predict_link(\n",
        "                    z_external[ext_idx:ext_idx+1], \n",
        "                    z_vendor[ven_idx:ven_idx+1]\n",
        "                ).item()\n",
        "                \n",
        "                if prob >= threshold:\n",
        "                    predicted_links.append({\n",
        "                        'external_idx': ext_idx, \n",
        "                        'vendor_idx': ven_idx, \n",
        "                        'probability': prob\n",
        "                    })\n",
        "    \n",
        "    return predicted_links\n",
        "\n",
        "\n",
        "def identify_bottlenecks(predicted_links, mappings, threshold_dependents=2):\n",
        "    \"\"\"\n",
        "    Identify bottlenecks - external suppliers that serve multiple vendors.\n",
        "    \n",
        "    A bottleneck is a single point of failure. If an external supplier\n",
        "    serves many vendors, its failure would have outsized impact.\n",
        "    \n",
        "    Args:\n",
        "        predicted_links: List of predicted (external â†’ vendor) links\n",
        "        mappings: Node ID mappings\n",
        "        threshold_dependents: Minimum vendors to be considered a bottleneck\n",
        "        \n",
        "    Returns:\n",
        "        List of bottlenecks sorted by dependent count (descending)\n",
        "    \"\"\"\n",
        "    # Count how many vendors each external supplier serves\n",
        "    external_dependents = defaultdict(list)\n",
        "    for link in predicted_links:\n",
        "        external_dependents[link['external_idx']].append(link['vendor_idx'])\n",
        "    \n",
        "    # Flag suppliers with multiple dependents\n",
        "    bottlenecks = []\n",
        "    for ext_idx, dependents in external_dependents.items():\n",
        "        if len(dependents) >= threshold_dependents:\n",
        "            bottlenecks.append({\n",
        "                'external_idx': ext_idx, \n",
        "                'dependent_count': len(dependents), \n",
        "                'dependent_vendors': dependents\n",
        "            })\n",
        "    \n",
        "    # Sort by impact (most dependents first)\n",
        "    bottlenecks.sort(key=lambda x: x['dependent_count'], reverse=True)\n",
        "    return bottlenecks\n",
        "\n",
        "\n",
        "# Execute link prediction\n",
        "print(\"=\" * 60)\n",
        "print(\"HIDDEN LINK PREDICTION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Scanning all (external, vendor) pairs for hidden relationships...\")\n",
        "\n",
        "# Using 0.3 threshold favors recall over precision - missing a real dependency is costlier than a false positive\n",
        "predicted_links = predict_hidden_links(model, embeddings, mappings, threshold=0.3)\n",
        "\n",
        "# Analyze predictions\n",
        "total_possible = len(mappings['external']) * len(mappings['vendor'])\n",
        "print(f\"Total possible pairs: {total_possible:,}\")\n",
        "print(f\"Predicted links (â‰¥30% confidence): {len(predicted_links):,}\")\n",
        "print(f\"Link density: {len(predicted_links)/total_possible*100:.2f}%\")\n",
        "\n",
        "# Identify bottlenecks\n",
        "bottlenecks = identify_bottlenecks(predicted_links, mappings, threshold_dependents=2)\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"ðŸš¨ BOTTLENECKS IDENTIFIED: {len(bottlenecks)}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "idx_to_external = {v: k for k, v in mappings['external'].items()}\n",
        "idx_to_vendor = {v: k for k, v in mappings['vendor'].items()}\n",
        "\n",
        "print(\"\\nTop 5 Single Points of Failure:\")\n",
        "for i, bn in enumerate(bottlenecks[:5]):\n",
        "    ext_name = idx_to_external.get(bn['external_idx'], f\"External-{bn['external_idx']}\")\n",
        "    print(f\"  {i+1}. {ext_name[:40]:<40s} â†’ {bn['dependent_count']:>3d} vendors depend on this supplier\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "predict_links_identify_bottlenecks"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BOTTLENECK AND LINK PREDICTION VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Link probability distribution\n",
        "ax = axes[0]\n",
        "link_probs = [l['probability'] for l in predicted_links]\n",
        "ax.hist(link_probs, bins=20, color='#3498db', edgecolor='white', alpha=0.8)\n",
        "ax.axvline(x=0.5, color='#e74c3c', linestyle='--', linewidth=2, label='Medium confidence')\n",
        "ax.axvline(x=0.7, color='#27ae60', linestyle='--', linewidth=2, label='High confidence')\n",
        "ax.set_xlabel('Link Probability')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Predicted Link Confidence Distribution', fontweight='bold')\n",
        "ax.legend()\n",
        "\n",
        "# Plot 2: Bottleneck impact distribution\n",
        "ax = axes[1]\n",
        "if bottlenecks:\n",
        "    dependent_counts = [bn['dependent_count'] for bn in bottlenecks]\n",
        "    ax.hist(dependent_counts, bins=range(2, max(dependent_counts)+2), \n",
        "            color='#e74c3c', edgecolor='white', alpha=0.8)\n",
        "    ax.set_xlabel('Number of Dependent Vendors')\n",
        "    ax.set_ylabel('Number of External Suppliers')\n",
        "    ax.set_title('Bottleneck Size Distribution', fontweight='bold')\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'No bottlenecks identified', ha='center', va='center', fontsize=12)\n",
        "    ax.set_title('Bottleneck Size Distribution', fontweight='bold')\n",
        "\n",
        "# Plot 3: Top bottlenecks bar chart\n",
        "ax = axes[2]\n",
        "if bottlenecks:\n",
        "    top_bn = bottlenecks[:10]\n",
        "    names = [idx_to_external.get(bn['external_idx'], f\"Ext-{bn['external_idx']}\")[:20] for bn in top_bn]\n",
        "    counts = [bn['dependent_count'] for bn in top_bn]\n",
        "    colors = ['#e74c3c' if c >= 5 else '#f39c12' if c >= 3 else '#27ae60' for c in counts]\n",
        "    ax.barh(range(len(names)), counts, color=colors)\n",
        "    ax.set_yticks(range(len(names)))\n",
        "    ax.set_yticklabels(names, fontsize=8)\n",
        "    ax.set_xlabel('Dependent Vendor Count')\n",
        "    ax.set_title('Top 10 Bottlenecks', fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/bottleneck_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nðŸ“Š Link Prediction Summary:\")\n",
        "print(f\"   Low confidence (30-50%): {sum(1 for l in predicted_links if 0.3 <= l['probability'] < 0.5)}\")\n",
        "print(f\"   Medium confidence (50-70%): {sum(1 for l in predicted_links if 0.5 <= l['probability'] < 0.7)}\")\n",
        "print(f\"   High confidence (â‰¥70%): {sum(1 for l in predicted_links if l['probability'] >= 0.7)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "bottleneck_visualization"
      },
      "source": [
        "### Interpreting Link Predictions: What the Probabilities Mean\n",
        "\n",
        "The link prediction scores require careful interpretation:\n",
        "\n",
        "**What the model is actually computing:**\n",
        "```python\n",
        "P(link exists) = sigmoid(embedding_external Â· embedding_vendor)\n",
        "```\n",
        "\n",
        "This dot product measures **similarity in embedding space**. High similarity â†’ high probability. But what does \"similar\" mean here?\n",
        "\n",
        "**Two nodes are similar if they have similar graph neighborhoods.** An external supplier and vendor are predicted to be connected if:\n",
        "- The external supplier ships to vendors that look like this vendor\n",
        "- The vendor receives from external suppliers that look like this one\n",
        "- They share common patterns in their 2-hop neighborhoods\n",
        "\n",
        "**Thresholds and confidence:**\n",
        "\n",
        "| Probability | Evidence | Interpretation |\n",
        "|-------------|----------|----------------|\n",
        "| 0.30 - 0.50 | WEAK | Model sees some similarity; could be noise |\n",
        "| 0.50 - 0.70 | MODERATE | Likely real relationship; worth investigating |\n",
        "| 0.70 - 1.00 | STRONG | High confidence hidden dependency |\n",
        "\n",
        "**Important caveat**: A high probability doesn't prove a relationship exists - it means the model found strong structural evidence. Always validate high-confidence predictions against business knowledge or additional data sources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "write_results_header"
      },
      "source": [
        "## 8. Write Results to Snowflake\n",
        "\n",
        "### Output Tables\n",
        "\n",
        "| Table | Description | Key Columns |\n",
        "|-------|-------------|-------------|\n",
        "| `RISK_SCORES` | Risk scores for all nodes | NODE_ID, NODE_TYPE, RISK_SCORE, RISK_CATEGORY, EMBEDDING |\n",
        "| `PREDICTED_LINKS` | Inferred supplier relationships | SOURCE_NODE_ID, TARGET_NODE_ID, PROBABILITY, EVIDENCE_STRENGTH |\n",
        "| `BOTTLENECKS` | Single points of failure | NODE_ID, DEPENDENT_COUNT, IMPACT_SCORE, MITIGATION_STATUS |\n",
        "\n",
        "The embeddings are stored as JSON arrays for potential downstream use (e.g., similarity search, clustering).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "write_risk_scores"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS FOR DATA OUTPUT\n",
        "# =============================================================================\n",
        "\n",
        "def categorize_risk(score):\n",
        "    \"\"\"\n",
        "    Categorize risk score into discrete buckets for business interpretation.\n",
        "    \n",
        "    Thresholds:\n",
        "        < 0.25: LOW - acceptable risk, standard monitoring\n",
        "        < 0.50: MEDIUM - elevated risk, enhanced monitoring recommended\n",
        "        < 0.75: HIGH - significant risk, mitigation planning required\n",
        "        >= 0.75: CRITICAL - immediate attention required\n",
        "    \"\"\"\n",
        "    if score < 0.25: return 'LOW'\n",
        "    elif score < 0.5: return 'MEDIUM'\n",
        "    elif score < 0.75: return 'HIGH'\n",
        "    else: return 'CRITICAL'\n",
        "\n",
        "def safe_float(val, default=0.0):\n",
        "    \"\"\"Convert value to float, handling inf/nan edge cases from model output.\"\"\"\n",
        "    if val is None or np.isnan(val) or np.isinf(val): \n",
        "        return default\n",
        "    return float(val)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# WRITE RISK SCORES TO SNOWFLAKE\n",
        "# =============================================================================\n",
        "\n",
        "def write_risk_scores(session, risk_scores, embeddings, mappings, model_version):\n",
        "    \"\"\"\n",
        "    Write risk scores to RISK_SCORES table.\n",
        "    \n",
        "    Each record contains:\n",
        "        - NODE_ID: Business identifier\n",
        "        - NODE_TYPE: SUPPLIER, PART, or EXTERNAL_SUPPLIER\n",
        "        - RISK_SCORE: Continuous score [0, 1]\n",
        "        - RISK_CATEGORY: Discrete category (LOW/MEDIUM/HIGH/CRITICAL)\n",
        "        - CONFIDENCE: Model confidence in the prediction\n",
        "        - EMBEDDING: 32-dimensional vector as JSON array\n",
        "        - MODEL_VERSION: For tracking which model produced the results\n",
        "    \"\"\"\n",
        "    # Delete previous results for this model version (idempotent updates)\n",
        "    session.sql(f\"DELETE FROM RISK_SCORES WHERE MODEL_VERSION = '{model_version}'\").collect()\n",
        "    \n",
        "    records = []\n",
        "    \n",
        "    # Reverse mappings: index â†’ business ID\n",
        "    idx_to_vendor = {v: k for k, v in mappings['vendor'].items()}\n",
        "    idx_to_material = {v: k for k, v in mappings['material'].items()}\n",
        "    idx_to_external = {v: k for k, v in mappings['external'].items()}\n",
        "    \n",
        "    # Process vendor risk scores (highest confidence - most data available)\n",
        "    # Confidence reflects data quality: internal vendors have rich data (0.85), externals are inferred (0.75)\n",
        "    if 'vendor' in risk_scores:\n",
        "        scores = risk_scores['vendor'].squeeze().cpu().numpy()\n",
        "        emb = embeddings['vendor'].cpu().numpy()\n",
        "        for idx, score in enumerate(scores):\n",
        "            records.append({\n",
        "                'NODE_ID': idx_to_vendor.get(idx, f'V-{idx}'), \n",
        "                'NODE_TYPE': 'SUPPLIER',\n",
        "                'RISK_SCORE': safe_float(score), \n",
        "                'RISK_CATEGORY': categorize_risk(score),\n",
        "                'CONFIDENCE': 0.85,  # High confidence for internal vendors\n",
        "                'EMBEDDING': str(emb[idx].tolist()), \n",
        "                'MODEL_VERSION': model_version\n",
        "            })\n",
        "    \n",
        "    # Process material risk scores (slightly lower confidence)\n",
        "    if 'material' in risk_scores:\n",
        "        scores = risk_scores['material'].squeeze().cpu().numpy()\n",
        "        emb = embeddings['material'].cpu().numpy()\n",
        "        for idx, score in enumerate(scores):\n",
        "            records.append({\n",
        "                'NODE_ID': idx_to_material.get(idx, f'M-{idx}'), \n",
        "                'NODE_TYPE': 'PART',\n",
        "                'RISK_SCORE': safe_float(score), \n",
        "                'RISK_CATEGORY': categorize_risk(score),\n",
        "                'CONFIDENCE': 0.80,  # Less direct risk data for materials\n",
        "                'EMBEDDING': str(emb[idx].tolist()), \n",
        "                'MODEL_VERSION': model_version\n",
        "            })\n",
        "    \n",
        "    # Process external supplier risk scores (lowest confidence - inferred from trade data)\n",
        "    if 'external' in risk_scores:\n",
        "        scores = risk_scores['external'].squeeze().cpu().numpy()\n",
        "        emb = embeddings['external'].cpu().numpy()\n",
        "        for idx, score in enumerate(scores):\n",
        "            records.append({\n",
        "                'NODE_ID': idx_to_external.get(idx, f'E-{idx}'), \n",
        "                'NODE_TYPE': 'EXTERNAL_SUPPLIER',\n",
        "                'RISK_SCORE': safe_float(score), \n",
        "                'RISK_CATEGORY': categorize_risk(score),\n",
        "                'CONFIDENCE': 0.75,  # Lower confidence for discovered suppliers\n",
        "                'EMBEDDING': str(emb[idx].tolist()), \n",
        "                'MODEL_VERSION': model_version\n",
        "            })\n",
        "    \n",
        "    # Write to Snowflake\n",
        "    if records:\n",
        "        df = pd.DataFrame(records)\n",
        "        session.write_pandas(df, 'RISK_SCORES', auto_create_table=False, overwrite=False)\n",
        "    \n",
        "    return len(records)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"WRITING RESULTS TO SNOWFLAKE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nðŸ“ Writing risk scores...\")\n",
        "num_scores = write_risk_scores(session, risk_scores, embeddings, mappings, MODEL_VERSION)\n",
        "print(f\"   âœ… Wrote {num_scores:,} risk score records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "write_links_and_bottlenecks"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# WRITE PREDICTED LINKS AND BOTTLENECKS\n",
        "# =============================================================================\n",
        "\n",
        "def write_predicted_links(session, predicted_links, mappings, model_version):\n",
        "    \"\"\"\n",
        "    Write predicted links to PREDICTED_LINKS table.\n",
        "    \n",
        "    Evidence strength categories:\n",
        "        WEAK: 30-50% probability - might be noise\n",
        "        MODERATE: 50-70% probability - likely real relationship\n",
        "        STRONG: 70%+ probability - high confidence hidden dependency\n",
        "    \"\"\"\n",
        "    session.sql(f\"DELETE FROM PREDICTED_LINKS WHERE MODEL_VERSION = '{model_version}'\").collect()\n",
        "    \n",
        "    idx_to_external = {v: k for k, v in mappings['external'].items()}\n",
        "    idx_to_vendor = {v: k for k, v in mappings['vendor'].items()}\n",
        "    \n",
        "    records = []\n",
        "    for link in predicted_links:\n",
        "        # Categorize evidence strength based on probability\n",
        "        if link['probability'] < 0.5:\n",
        "            evidence = 'WEAK'\n",
        "        elif link['probability'] < 0.7:\n",
        "            evidence = 'MODERATE'\n",
        "        else:\n",
        "            evidence = 'STRONG'\n",
        "            \n",
        "        records.append({\n",
        "            'SOURCE_NODE_ID': idx_to_external.get(link['external_idx'], f\"E-{link['external_idx']}\"),\n",
        "            'SOURCE_NODE_TYPE': 'EXTERNAL_SUPPLIER',\n",
        "            'TARGET_NODE_ID': idx_to_vendor.get(link['vendor_idx'], f\"V-{link['vendor_idx']}\"),\n",
        "            'TARGET_NODE_TYPE': 'SUPPLIER', \n",
        "            'LINK_TYPE': 'INFERRED_SUPPLIES',\n",
        "            'PROBABILITY': safe_float(link['probability']), \n",
        "            'EVIDENCE_STRENGTH': evidence, \n",
        "            'MODEL_VERSION': model_version\n",
        "        })\n",
        "    \n",
        "    if records:\n",
        "        df = pd.DataFrame(records)\n",
        "        session.write_pandas(df, 'PREDICTED_LINKS', auto_create_table=False, overwrite=False)\n",
        "    return len(records)\n",
        "\n",
        "\n",
        "def write_bottlenecks(session, bottlenecks, mappings):\n",
        "    \"\"\"\n",
        "    Write identified bottlenecks to BOTTLENECKS table.\n",
        "    \n",
        "    Impact scoring:\n",
        "        - Scales linearly with dependent count\n",
        "        - Capped at 1.0 for 10+ dependents\n",
        "        - Used for prioritizing mitigation efforts\n",
        "    \"\"\"\n",
        "    # Only delete unmitigated bottlenecks (preserve mitigation history)\n",
        "    # Preserving mitigated records maintains audit trail and prevents re-alerting on resolved issues\n",
        "    session.sql(\"DELETE FROM BOTTLENECKS WHERE MITIGATION_STATUS = 'UNMITIGATED'\").collect()\n",
        "    \n",
        "    idx_to_external = {v: k for k, v in mappings['external'].items()}\n",
        "    idx_to_vendor = {v: k for k, v in mappings['vendor'].items()}\n",
        "    \n",
        "    records = []\n",
        "    for bn in bottlenecks:\n",
        "        ext_name = idx_to_external.get(bn['external_idx'], f\"External-{bn['external_idx']}\")\n",
        "        dependent_names = [idx_to_vendor.get(v, f\"V-{v}\") for v in bn['dependent_vendors']]\n",
        "        \n",
        "        # Impact score: linear scale capped at 1.0\n",
        "        impact = min(1.0, bn['dependent_count'] / 10.0)\n",
        "        \n",
        "        records.append({\n",
        "            'NODE_ID': ext_name, \n",
        "            'NODE_TYPE': 'EXTERNAL_SUPPLIER', \n",
        "            'DEPENDENT_COUNT': bn['dependent_count'],\n",
        "            'DEPENDENT_NODES': str(dependent_names),  # JSON array of vendor IDs\n",
        "            'IMPACT_SCORE': safe_float(impact),\n",
        "            'DESCRIPTION': f\"External supplier '{ext_name}' is a single point of failure for {bn['dependent_count']} vendors\",\n",
        "            'MITIGATION_STATUS': 'UNMITIGATED'  # Default status for new bottlenecks\n",
        "        })\n",
        "    \n",
        "    if records:\n",
        "        df = pd.DataFrame(records)\n",
        "        session.write_pandas(df, 'BOTTLENECKS', auto_create_table=False, overwrite=False)\n",
        "    return len(records)\n",
        "\n",
        "\n",
        "print(\"\\nðŸ“ Writing predicted links...\")\n",
        "num_links = write_predicted_links(session, predicted_links, mappings, MODEL_VERSION)\n",
        "print(f\"   âœ… Wrote {num_links:,} predicted link records\")\n",
        "\n",
        "print(\"\\nðŸ“ Writing bottlenecks...\")\n",
        "num_bn = write_bottlenecks(session, bottlenecks, mappings)\n",
        "print(f\"   âœ… Wrote {num_bn:,} bottleneck records\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ALL RESULTS WRITTEN TO SNOWFLAKE\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "summary_header"
      },
      "source": [
        "## 9. Summary & Verification\n",
        "\n",
        "Query the output tables to verify results and provide a final summary of the analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "display_analysis_summary"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY & VERIFICATION\n",
        "# =============================================================================\n",
        "# Querying Snowflake tables validates the write succeeded and provides a sanity check on results\n",
        "\n",
        "print(\"â•”\" + \"â•\"*58 + \"â•—\")\n",
        "print(\"â•‘\" + \" GNN SUPPLY CHAIN RISK ANALYSIS - FINAL SUMMARY \".center(58) + \"â•‘\")\n",
        "print(\"â•š\" + \"â•\"*58 + \"â•\")\n",
        "\n",
        "# Query and display risk score summary from Snowflake\n",
        "risk_summary = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        NODE_TYPE, \n",
        "        COUNT(*) as COUNT, \n",
        "        ROUND(AVG(RISK_SCORE), 3) as AVG_RISK,\n",
        "        SUM(CASE WHEN RISK_CATEGORY = 'CRITICAL' THEN 1 ELSE 0 END) as CRITICAL,\n",
        "        SUM(CASE WHEN RISK_CATEGORY = 'HIGH' THEN 1 ELSE 0 END) as HIGH,\n",
        "        SUM(CASE WHEN RISK_CATEGORY = 'MEDIUM' THEN 1 ELSE 0 END) as MEDIUM,\n",
        "        SUM(CASE WHEN RISK_CATEGORY = 'LOW' THEN 1 ELSE 0 END) as LOW\n",
        "    FROM RISK_SCORES \n",
        "    WHERE MODEL_VERSION = '{}'\n",
        "    GROUP BY NODE_TYPE\n",
        "    ORDER BY AVG_RISK DESC\n",
        "\"\"\".format(MODEL_VERSION)).to_pandas()\n",
        "\n",
        "print(\"\\nðŸ“Š RISK SCORE SUMMARY BY NODE TYPE\")\n",
        "print(\"-\" * 60)\n",
        "print(risk_summary.to_string(index=False))\n",
        "\n",
        "# Top bottlenecks from Snowflake\n",
        "top_bottlenecks = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        NODE_ID, \n",
        "        DEPENDENT_COUNT, \n",
        "        ROUND(IMPACT_SCORE, 2) as IMPACT_SCORE \n",
        "    FROM BOTTLENECKS\n",
        "    WHERE MITIGATION_STATUS = 'UNMITIGATED'\n",
        "    ORDER BY IMPACT_SCORE DESC \n",
        "    LIMIT 5\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "print(\"\\nðŸš¨ TOP 5 BOTTLENECKS (Single Points of Failure)\")\n",
        "print(\"-\" * 60)\n",
        "if len(top_bottlenecks) > 0:\n",
        "    print(top_bottlenecks.to_string(index=False))\n",
        "else:\n",
        "    print(\"No bottlenecks identified above threshold\")\n",
        "\n",
        "# High confidence predicted links\n",
        "high_conf_links = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        SOURCE_NODE_ID, \n",
        "        TARGET_NODE_ID, \n",
        "        ROUND(PROBABILITY, 3) as PROBABILITY,\n",
        "        EVIDENCE_STRENGTH\n",
        "    FROM PREDICTED_LINKS\n",
        "    WHERE PROBABILITY >= 0.5 \n",
        "    AND MODEL_VERSION = '{}'\n",
        "    ORDER BY PROBABILITY DESC \n",
        "    LIMIT 10\n",
        "\"\"\".format(MODEL_VERSION)).to_pandas()\n",
        "\n",
        "print(\"\\nðŸ”— TOP 10 HIGH-CONFIDENCE PREDICTED LINKS\")\n",
        "print(\"-\" * 60)\n",
        "if len(high_conf_links) > 0:\n",
        "    print(high_conf_links.to_string(index=False))\n",
        "else:\n",
        "    print(\"No high-confidence links above 50% threshold\")\n",
        "\n",
        "# Final statistics\n",
        "print(\"\\n\" + \"â•\" * 60)\n",
        "print(\"ðŸ“ˆ ANALYSIS STATISTICS\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"   Model Version: {MODEL_VERSION}\")\n",
        "print(f\"   Total nodes analyzed: {sum(len(m) for m in mappings.values()):,}\")\n",
        "print(f\"   Total edges in graph: {sum(e.shape[1] for e in edge_indices.values()):,}\")\n",
        "print(f\"   Risk scores generated: {num_scores:,}\")\n",
        "print(f\"   Hidden links predicted: {num_links:,}\")\n",
        "print(f\"   Bottlenecks identified: {num_bn:,}\")\n",
        "print(\"â•\" * 60)\n",
        "\n",
        "print(\"\\nâœ… Analysis complete! Results are available in Snowflake tables:\")\n",
        "print(\"   â€¢ RISK_SCORES\")\n",
        "print(\"   â€¢ PREDICTED_LINKS\")  \n",
        "print(\"   â€¢ BOTTLENECKS\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
